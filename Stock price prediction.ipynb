{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985bb62-c275-437c-93a5-e5f063a663f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC \n",
    "from xgboost import XGBClassifier \n",
    "from sklearn import metrics \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2e407-7d01-45fa-b3e9-54a759f2f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TESLA (2).csv') \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658c5f9-ec76-403c-9f15-90590f2dcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the first five rows, we can see that data for some of the dates is missing the reason for that is on weekends and holidays Stock Market remains closed hence no trading happens on these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a6ad0-3cd7-4501-90ea-722027982a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f38fc-9a88-4a7a-9509-5a30f285b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5526385-3e87-4458-944d-7672868a076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18606737-0e79-4ea5-b864-2c9de4c1e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) \n",
    "plt.plot(df['Close']) \n",
    "plt.title('Tesla Close price.', fontsize=15) \n",
    "plt.ylabel('Price in dollars.') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee692ae-f2bb-4922-9eb0-cda8fe90fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The prices of tesla stocks are showing an upward and downward trend as depicted by the plot of the closing price of the stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fccdcb-3d76-49c2-8a50-bc8ab39e97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80597e33-4575-4cbc-ba11-39ea0780c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we observe carefully we can see that the data in the ‘Close’ column and that available in the ‘Adj Close’ column is the same let’s check whether this is the case with each row or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a407a6-b6be-4447-a83e-98436de1da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Close']==df['Adj Close']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08e979-408b-4776-b968-5fb429138cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From here we can conclude that all the rows of columns ‘Close’ and ‘Adj Close’ have the same data. So, having redundant data in the dataset is not going to help so, we’ll drop this column before further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83571e45-0ca4-4cd1-8962-d1d496ba9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Adj Close'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9246d-5a8a-46b7-8ad8-be1881f3a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let’s draw the distribution plot for the continuous features given in the dataset.\n",
    "\n",
    "#Before moving further let’s check for the null values if any are present in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4286ad-882c-4fe1-a801-98a45f73a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dade236-d5d0-4c3e-b94d-eb9ffa92fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This implies that there are no null values in the data set provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30090aa0-585f-471d-9cac-591fcefc5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['Open','High','Low','Close','Volume']\n",
    "for i, col in enumerate(features):\n",
    " plt.subplot(2,3,i+1)\n",
    " sb.distplot(df[col])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ab8d0-f828-4509-ba08-9faff73b5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the distribution plot of OHLC data, we can see two peaks which means the data has varied significantly in two regions. And the Volume data is left-skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e7371-0c24-46d7-bb4c-ba47d4321563",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,10)) \n",
    "for i, col in enumerate(features):\n",
    " plt.subplot(2,3,i+1)\n",
    " sb.boxplot(df[col])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fd4aa-fc8a-4173-a61e-7ea3eaf3d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above boxplots, we can conclude that only volume data contains outliers in it but the data in the rest of the columns are free from any outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea6538-b10b-4b0d-b9fa-b70b1b157b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering helps to derive some valuable features from the existing ones. These extra features sometimes help in increasing the performance of the model significantly and certainly help to gain deeper insights into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e756a-856a-421a-abe0-8cab7218b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = df['Date'].str.split('/',expand=True) \n",
    " \n",
    "\n",
    "df['day'] = splitted[1].astype('int') \n",
    "df['month'] = splitted[0].astype('int') \n",
    "df['year'] = splitted[2].astype('int') \n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f084f5-1ad5-4a73-b870-a48b5bd69f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_quarter_end'] = np.where(df['month']%3==0,1,0) \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5d987-1547-415f-8959-468eb033db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped = df.drop('Date', axis=1).groupby('year').mean()\n",
    "plt.subplots(figsize=(20,10))\n",
    "\n",
    "for i, col in enumerate(['Open', 'High', 'Low', 'Close']):\n",
    "  plt.subplot(2,2,i+1)\n",
    "  data_grouped[col].plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7c13f-ee25-426b-8e32-378d732a12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Date', axis=1).groupby('is_quarter_end').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb086c-74b4-4cc7-9fec-3d1ba07347d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A quarter is defined as a group of three months. Every company prepares its quarterly results and publishes them publicly so, that people can analyze the company’s performance. These quarterly results affect the stock prices heavily which is why we have added this feature because this can be a helpful feature for the learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18209762-18e9-4757-a20f-cc16321e4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are some of the important observations of the above-grouped data:\n",
    "\n",
    "#Prices are higher in the months which are quarter end as compared to that of the non-quarter end months.\n",
    "#The volume of trades is lower in the months which are quarter end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4949e-9b24-409c-9d63-658ca57bfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['open-close'] = df['Open'] - df['Close'] \n",
    "df['low-high'] = df['Low'] - df['High'] \n",
    "df['target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec3d4b-24c8-4572-9e26-afa7ff1b9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['target'].value_counts().values, \n",
    "\t\tlabels=[0, 1], autopct='%1.1f%%') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056883fd-b883-49fe-9104-01c57a846031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When we add features to our dataset we have to ensure that there are no highly correlated features as they do not help in the learning process of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05834a8-94d4-4f1f-a365-9c9d2e4798ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation matrix on the numerical columns\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(5, 5))\n",
    "sb.heatmap(corr_matrix > 0.9, annot=True, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630750cd-076e-49d8-aaa7-58f183cb6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above heatmap, we can say that there is a high correlation between OHLC that is pretty obvious, and the added features are not highly correlated with each other or previously provided features which means that we are good to go and build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ae996-7eb7-4147-a7fc-53eb414d7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA SPLITTING AND NORMALIZATION\n",
    "features = df[['open-close', 'low-high', 'is_quarter_end']] \n",
    "target = df['target'] \n",
    "\n",
    "scaler = StandardScaler() \n",
    "features = scaler.fit_transform(features) \n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split( \n",
    "\tfeatures, target, test_size=0.1, random_state=2022) \n",
    "print(X_train.shape, X_valid.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51d747-3ed5-4395-aca9-3cc788973ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After selecting the features to train the model on we should normalize the data because normalized data leads to stable and fast training of the model. After that whole data has been split into two parts with a 90/10 ratio so, that we can evaluate the performance of our model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be58ee-c2aa-4246-b6b7-674825d9ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), SVC( \n",
    "  kernel='poly', probability=True), XGBClassifier()] \n",
    "  \n",
    "for i in range(3): \n",
    "  models[i].fit(X_train, Y_train) \n",
    "  \n",
    "  print(f'{models[i]} : ') \n",
    "  print('Training Accuracy : ', metrics.roc_auc_score( \n",
    "    Y_train, models[i].predict_proba(X_train)[:,1])) \n",
    "  print('Validation Accuracy : ', metrics.roc_auc_score( \n",
    "    Y_valid, models[i].predict_proba(X_valid)[:,1])) \n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4de000-ca32-4a95-ae74-9f4d61f24343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Among the three models, we have trained XGBClassifier has the highest performance but it is pruned to overfitting as the difference between the training and the validation accuracy is too high. But in the case of the Logistic Regression, this is not the case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14f493-5e08-4bfd-b535-5291f0584911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming models[1] is your classifier and xtest, ytest are your test data\n",
    "y_pred = models[1].predict(X_valid)\n",
    "\n",
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(models[1], X_valid, Y_valid)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
